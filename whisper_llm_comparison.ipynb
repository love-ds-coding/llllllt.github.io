{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dfed2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Whisper + LLM Model Comparison Script\n",
    "\n",
    "Compare different Whisper backends and Ollama models for audio transcription and summarization.\n",
    "\n",
    "Features:\n",
    "- Multiple Whisper backends (faster-whisper, openai-whisper)\n",
    "- Multiple Ollama models (llama3.1:8b, mistral:7b, etc.)\n",
    "- Timing measurements for each stage\n",
    "- Quality metrics (WER if reference provided)\n",
    "- Export results to CSV\n",
    "\n",
    "Setup:\n",
    "1. Install required packages: pip install -r notebook_requirements.txt\n",
    "2. Ensure Ollama is running locally\n",
    "3. Set your audio file paths and model preferences below\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe8385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a6fd7c",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "CONFIGURATION - MODIFY THESE VARIABLES\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ead15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio file paths (use absolute paths or relative to script)\n",
    "AUDIO_PATHS = [\n",
    "    \"./sample_audio_1.wav\",  # Replace with your audio files\n",
    "    \"./sample_audio_2.mp3\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b28da94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference texts for WER calculation (optional, same length as AUDIO_PATHS)\n",
    "# Leave as None if no reference available\n",
    "REFERENCE_TEXTS = [\n",
    "    None,  # No reference for first file\n",
    "    \"This is a reference transcript for the second audio file.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f84bc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whisper models to test\n",
    "WHISPER_MODELS = [\n",
    "    \"tiny\",      # fastest, lowest quality\n",
    "    \"base\",      # fast, decent quality\n",
    "    \"small\",     # balanced\n",
    "    \"medium\",    # higher quality, slower\n",
    "    \"large-v3\",  # best quality, slowest\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673faca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama models to test\n",
    "OLLAMA_MODELS = [\n",
    "    \"llama3.1:8b\",\n",
    "    \"mistral:7b\",\n",
    "    \"llama3.1:1b\",\n",
    "    \"qwen2.5:7b\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a524fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama base URL (change if running on different host/port)\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cc1cb4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Summary prompt template\n",
    "SUMMARY_PROMPT_TEMPLATE = \"Summarize this transcript in 2-3 sentences:\\n\\n{transcript}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219eb11e",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "HELPER FUNCTIONS\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4037709",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def transcribe_with_faster_whisper(audio_path: str, model_size: str) -> Tuple[str, float]:\n",
    "    \"\"\"Transcribe audio using faster-whisper\"\"\"\n",
    "    try:\n",
    "        from faster_whisper import WhisperModel\n",
    "        model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")\n",
    "        start_time = time.time()\n",
    "        segments, info = model.transcribe(audio_path, beam_size=5)\n",
    "        transcript = \" \".join([segment.text for segment in segments])\n",
    "        duration = time.time() - start_time\n",
    "        return transcript.strip(), duration\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad891e0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def transcribe_with_openai_whisper(audio_path: str, model_size: str) -> Tuple[str, float]:\n",
    "    \"\"\"Transcribe audio using openai-whisper\"\"\"\n",
    "    try:\n",
    "        import whisper\n",
    "        model = whisper.load_model(model_size)\n",
    "        start_time = time.time()\n",
    "        result = model.transcribe(audio_path)\n",
    "        transcript = result[\"text\"]\n",
    "        duration = time.time() - start_time\n",
    "        return transcript.strip(), duration\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c096f65",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def summarize_with_ollama(text: str, model_name: str, base_url: str) -> Tuple[str, float]:\n",
    "    \"\"\"Summarize text using Ollama\"\"\"\n",
    "    try:\n",
    "        prompt = SUMMARY_PROMPT_TEMPLATE.format(transcript=text)\n",
    "        payload = {\n",
    "            \"model\": model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        start_time = time.time()\n",
    "        response = requests.post(f\"{base_url}/api/generate\", json=payload, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        result = response.json()\n",
    "        summary = result.get(\"response\", \"No response\")\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        return summary.strip(), duration\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5452bea4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def calculate_wer(reference: str, hypothesis: str) -> Optional[float]:\n",
    "    \"\"\"Calculate Word Error Rate between reference and hypothesis\"\"\"\n",
    "    if not reference or not hypothesis or 'Error:' in hypothesis:\n",
    "        return None\n",
    "    try:\n",
    "        from jiwer import wer\n",
    "        return wer(reference, hypothesis)\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfced36",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def check_ollama_model_availability(model_name: str, base_url: str) -> bool:\n",
    "    \"\"\"Check if an Ollama model is available\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{base_url}/api/tags\", timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            models = response.json().get(\"models\", [])\n",
    "            return any(model[\"name\"] == model_name for model in models)\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e51c9ec",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def add_custom_whisper_backend(name: str, transcribe_func):\n",
    "    \"\"\"Add a custom Whisper backend\"\"\"\n",
    "    global whisper_backends\n",
    "    whisper_backends[name] = transcribe_func\n",
    "    print(f\"✅ Added custom Whisper backend: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10518a9e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def add_custom_ollama_model(name: str):\n",
    "    \"\"\"Add a custom Ollama model to the test list\"\"\"\n",
    "    global OLLAMA_MODELS\n",
    "    if name not in OLLAMA_MODELS:\n",
    "        OLLAMA_MODELS.append(name)\n",
    "        print(f\"✅ Added Ollama model: {name}\")\n",
    "    else:\n",
    "        print(f\"⚠️  Model {name} already in list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d44269d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def add_custom_whisper_model_size(size: str):\n",
    "    \"\"\"Add a custom Whisper model size to test\"\"\"\n",
    "    global WHISPER_MODELS\n",
    "    if size not in WHISPER_MODELS:\n",
    "        WHISPER_MODELS.append(size)\n",
    "        print(f\"✅ Added Whisper model size: {size}\")\n",
    "    else:\n",
    "        print(f\"⚠️  Model size {size} already in list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597bd13d",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "MAIN COMPARISON FUNCTION\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfa851e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_comparison() -> List[Dict]:\n",
    "    \"\"\"Run the full comparison across all models and audio files\"\"\"\n",
    "    \n",
    "    if not whisper_backends or not available_ollama_models or not valid_audio_paths:\n",
    "        print(\"❌ Cannot run comparison - missing required components\")\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    total_tests = len(whisper_backends) * len(WHISPER_MODELS) * len(available_ollama_models) * len(valid_audio_paths)\n",
    "    current_test = 0\n",
    "    \n",
    "    print(f\"🚀 Starting comparison ({total_tests} total tests)...\")\n",
    "    \n",
    "    for audio_idx, audio_path in enumerate(valid_audio_paths):\n",
    "        audio_name = os.path.basename(audio_path)\n",
    "        reference_text = REFERENCE_TEXTS[audio_idx] if audio_idx < len(REFERENCE_TEXTS) else None\n",
    "        \n",
    "        print(f\"\\n🎵 Processing audio: {audio_name}\")\n",
    "        \n",
    "        for whisper_backend_name, whisper_backend in whisper_backends.items():\n",
    "            for whisper_model_size in WHISPER_MODELS:\n",
    "                \n",
    "                # Skip if model size not supported by backend\n",
    "                if whisper_backend_name == 'faster-whisper' and whisper_model_size not in ['tiny', 'base', 'small', 'medium', 'large-v2', 'large-v3']:\n",
    "                    continue\n",
    "                if whisper_backend_name == 'openai-whisper' and whisper_model_size not in ['tiny', 'base', 'small', 'medium', 'large']:\n",
    "                    continue\n",
    "                \n",
    "                print(f\"  🎤 {whisper_backend_name} ({whisper_model_size})\")\n",
    "                \n",
    "                # Transcribe\n",
    "                if whisper_backend_name == 'faster-whisper':\n",
    "                    transcript, transcribe_time = transcribe_with_faster_whisper(audio_path, whisper_model_size)\n",
    "                else:\n",
    "                    transcript, transcribe_time = transcribe_with_openai_whisper(audio_path, whisper_model_size)\n",
    "                \n",
    "                # Skip if transcription failed\n",
    "                if 'Error:' in transcript:\n",
    "                    print(f\"    ❌ Transcription failed: {transcript}\")\n",
    "                    continue\n",
    "                \n",
    "                for ollama_model in available_ollama_models:\n",
    "                    current_test += 1\n",
    "                    print(f\"    🤖 {ollama_model} ({current_test}/{total_tests})\")\n",
    "                    \n",
    "                    # Summarize\n",
    "                    summary, summarize_time = summarize_with_ollama(transcript, ollama_model, OLLAMA_BASE_URL)\n",
    "                    \n",
    "                    # Calculate metrics\n",
    "                    transcript_length = len(transcript.split())\n",
    "                    summary_length = len(summary.split()) if 'Error:' not in summary else 0\n",
    "                    \n",
    "                    # Calculate WER if reference available\n",
    "                    wer_score = None\n",
    "                    if reference_text:\n",
    "                        wer_score = calculate_wer(reference_text, transcript)\n",
    "                    \n",
    "                    # Store results\n",
    "                    result = {\n",
    "                        'audio_file': audio_name,\n",
    "                        'whisper_backend': whisper_backend_name,\n",
    "                        'whisper_model': whisper_model_size,\n",
    "                        'ollama_model': ollama_model,\n",
    "                        'transcribe_time': round(transcribe_time, 3),\n",
    "                        'summarize_time': round(summarize_time, 3),\n",
    "                        'total_time': round(transcribe_time + summarize_time, 3),\n",
    "                        'transcript_length': transcript_length,\n",
    "                        'summary_length': summary_length,\n",
    "                        'wer_score': round(wer_score, 4) if wer_score is not None else None,\n",
    "                        'transcript_preview': transcript[:100] + '...' if len(transcript) > 100 else transcript,\n",
    "                        'summary_preview': summary[:100] + '...' if len(summary) > 100 else summary,\n",
    "                        'timestamp': datetime.now().isoformat(),\n",
    "                    }\n",
    "                    \n",
    "                    results.append(result)\n",
    "                    \n",
    "                    # Print progress\n",
    "                    status = \"✅\" if 'Error:' not in summary else \"⚠️\"\n",
    "                    print(f\"      {status} {result['total_time']}s | {transcript_length} words | {summary_length} words\")\n",
    "    \n",
    "    print(f\"\\n🎉 Comparison complete! {len(results)} successful tests\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d7a4c5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def display_results(results: List[Dict]):\n",
    "    \"\"\"Display results as a comparison table\"\"\"\n",
    "    if not results:\n",
    "        print(\"❌ No results to display\")\n",
    "        return\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Reorder columns for better readability\n",
    "    column_order = [\n",
    "        'audio_file', 'whisper_backend', 'whisper_model', 'ollama_model',\n",
    "        'total_time', 'transcribe_time', 'summarize_time',\n",
    "        'transcript_length', 'summary_length', 'wer_score',\n",
    "        'transcript_preview', 'summary_preview'\n",
    "    ]\n",
    "    df = df[column_order]\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(\"📊 Summary Statistics:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Fastest combinations\n",
    "    fastest = df.loc[df['total_time'].idxmin()]\n",
    "    print(f\"🏃 Fastest: {fastest['whisper_backend']} ({fastest['whisper_model']}) + {fastest['ollama_model']}\")\n",
    "    print(f\"   Time: {fastest['total_time']}s | Audio: {fastest['audio_file']}\")\n",
    "    \n",
    "    # Best quality (lowest WER)\n",
    "    wer_results = df[df['wer_score'].notna()]\n",
    "    if not wer_results.empty:\n",
    "        best_quality = wer_results.loc[wer_results['wer_score'].idxmin()]\n",
    "        print(f\"\\n🎯 Best Quality: {best_quality['whisper_backend']} ({best_quality['whisper_model']}) + {best_quality['ollama_model']}\")\n",
    "        print(f\"   WER: {best_quality['wer_score']:.4f} | Audio: {best_quality['audio_file']}\")\n",
    "    \n",
    "    # Model performance comparison\n",
    "    print(f\"\\n📈 Model Performance:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Whisper performance\n",
    "    whisper_stats = df.groupby(['whisper_backend', 'whisper_model'])['transcribe_time'].agg(['mean', 'min', 'max']).round(3)\n",
    "    print(\"Whisper Models (avg transcribe time):\")\n",
    "    print(whisper_stats)\n",
    "    \n",
    "    # Ollama performance\n",
    "    ollama_stats = df.groupby('ollama_model')['summarize_time'].agg(['mean', 'min', 'max']).round(3)\n",
    "    print(\"\\nOllama Models (avg summarize time):\")\n",
    "    print(ollama_stats)\n",
    "    \n",
    "    # Display full results table\n",
    "    print(f\"\\n📋 Full Results Table ({len(df)} rows):\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Format display\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', None)\n",
    "    pd.set_option('display.max_colwidth', 50)\n",
    "    \n",
    "    print(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa78da3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def export_results(results: List[Dict], filename: str = None):\n",
    "    \"\"\"Export results to CSV\"\"\"\n",
    "    if not results:\n",
    "        print(\"❌ No results to export\")\n",
    "        return None\n",
    "    \n",
    "    # Create filename with timestamp if not provided\n",
    "    if not filename:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"whisper_llm_comparison_{timestamp}.csv\"\n",
    "    \n",
    "    # Export to CSV\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(filename, index=False)\n",
    "    \n",
    "    print(f\"💾 Results exported to: {filename}\")\n",
    "    print(f\"📁 File size: {os.path.getsize(filename)} bytes\")\n",
    "    \n",
    "    # Show file location\n",
    "    abs_path = os.path.abspath(filename)\n",
    "    print(f\"📍 Absolute path: {abs_path}\")\n",
    "    \n",
    "    # Preview first few lines\n",
    "    print(f\"\\n📖 CSV Preview (first 3 rows):\")\n",
    "    print(\"-\" * 50)\n",
    "    with open(filename, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i < 4:  # Header + 3 data rows\n",
    "                print(line.strip())\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b5ccf3",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "MAIN EXECUTION\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b706700",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"🔧 Setting up Whisper + LLM comparison...\")\n",
    "    \n",
    "    # Try to import Whisper backends (some may not be installed)\n",
    "    whisper_backends = {}\n",
    "    \n",
    "    try:\n",
    "        from faster_whisper import WhisperModel\n",
    "        whisper_backends['faster-whisper'] = WhisperModel\n",
    "        print(\"✅ faster-whisper imported\")\n",
    "    except ImportError:\n",
    "        print(\"❌ faster-whisper not installed (pip install faster-whisper)\")\n",
    "    \n",
    "    try:\n",
    "        import whisper\n",
    "        whisper_backends['openai-whisper'] = whisper\n",
    "        print(\"✅ openai-whisper imported\")\n",
    "    except ImportError:\n",
    "        print(\"❌ openai-whisper not installed (pip install openai-whisper)\")\n",
    "    \n",
    "    # Check if we have any Whisper backends\n",
    "    if not whisper_backends:\n",
    "        print(\"\\n⚠️  No Whisper backends available! Install at least one:\")\n",
    "        print(\"   pip install faster-whisper  # Recommended\")\n",
    "        print(\"   pip install openai-whisper\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Check Ollama models\n",
    "    print(f\"\\n🔍 Checking Ollama models...\")\n",
    "    available_ollama_models = []\n",
    "    for model in OLLAMA_MODELS:\n",
    "        if check_ollama_model_availability(model, OLLAMA_BASE_URL):\n",
    "            available_ollama_models.append(model)\n",
    "            print(f\"✅ Ollama model available: {model}\")\n",
    "        else:\n",
    "            print(f\"❌ Ollama model not available: {model}\")\n",
    "    \n",
    "    if not available_ollama_models:\n",
    "        print(\"\\n⚠️  No Ollama models available! Make sure Ollama is running.\")\n",
    "        print(f\"   Check: {OLLAMA_BASE_URL}/api/tags\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Check audio files\n",
    "    print(f\"\\n🔍 Checking audio files...\")\n",
    "    valid_audio_paths = []\n",
    "    for i, path in enumerate(AUDIO_PATHS):\n",
    "        if os.path.exists(path):\n",
    "            valid_audio_paths.append(path)\n",
    "            print(f\"✅ Audio file exists: {path}\")\n",
    "        else:\n",
    "            print(f\"❌ Audio file not found: {path}\")\n",
    "    \n",
    "    if not valid_audio_paths:\n",
    "        print(\"\\n⚠️  No valid audio files found! Update AUDIO_PATHS above.\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(f\"\\n📊 Configuration:\")\n",
    "    print(f\"   Audio files: {len(valid_audio_paths)}\")\n",
    "    print(f\"   Whisper backends: {len(whisper_backends)}\")\n",
    "    print(f\"   Whisper models: {len(WHISPER_MODELS)}\")\n",
    "    print(f\"   Ollama models: {len(available_ollama_models)}\")\n",
    "    print(f\"   Ollama URL: {OLLAMA_BASE_URL}\")\n",
    "    \n",
    "    # Run the comparison\n",
    "    print(f\"\\n🚀 Starting comparison...\")\n",
    "    comparison_results = run_comparison()\n",
    "    \n",
    "    if comparison_results:\n",
    "        # Display results\n",
    "        print(f\"\\n📊 Displaying results...\")\n",
    "        df = display_results(comparison_results)\n",
    "        \n",
    "        # Export results\n",
    "        print(f\"\\n💾 Exporting results...\")\n",
    "        export_results(comparison_results)\n",
    "        \n",
    "        print(f\"\\n✅ All done! Check the CSV file for detailed results.\")\n",
    "    else:\n",
    "        print(f\"\\n❌ No results generated. Check the configuration and try again.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
